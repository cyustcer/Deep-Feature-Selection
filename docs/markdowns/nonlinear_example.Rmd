---
title: "Nonlinear example"
author: "Yao Chen"
date: "7/20/2020"
output: html_document
---

# Deep Feature Selection

In this markdown, we will demonstrate the comparison methods that are implemented in Table 2 of our paper.

## User Guide on nonlinear example

In this example, ...

The following methods will be implemented:

* Generalized Additive Model(GAM)
* Random Forest(RF)
* Bayesian Additive Regression Trees(BART)
* SCAD
* Bayesian Neural Networks(BNN)

### Data Preparation
In this section, we will read in the data that is generated using `nonlinear_generator` from `./src/utils.py`

```{r}
source("../../src/utils.R")
```


```{r}
dirc = "../../data/nonlinear/p_500_N_600_s_4/"
k = 0 # dataset index from 0 to 9
X <- read.table(paste(dirc, 'X_', toString(k), '.txt', sep=""))
y <- read.table(paste(dirc, 'y_', toString(k), '.txt', sep=""))
train_pos_idx = which(y == 1)[1:150] # take 150 positive observations for training set
train_neg_idx = which(y == 0)[1:150] # take 150 negative observations for training set
test_pos_idx = which(y == 1)[151:300] # take rest positive observations for test set
test_neg_idx = which(y == 0)[151:300] # take rest negative observations for test set
train_idx = sort(cbind(train_pos_idx, train_neg_idx)) # bind training sample index
test_idx = sort(cbind(test_pos_idx, test_neg_idx)) # bind test sample index
X_train = X[train_idx,] # extract training set
y_train = y[train_idx,] # extract test set
X_test = X[test_idx,]
y_test = y[test_idx,]
N = dim(X_train)[1]
p = dim(X_train)[2]
```

The dimension of training set: (`r dim(X_train)`)

* The number of positive samples: `r length(y_train==1)`
* The number of negative samples: `r length(y_train==0)`

The dimension of test set: (`r dim(X_test)`)

* The number of positive samples: `r length(y_test==1)`
* The number of negative samples: `r length(y_test==0)`

### Generalized Additive Model (GAM)

In this section, we will implement Generalized Additiveve Model(GAM) for variable selections and predictive preformance. We will use R package `gamsel`. We first use function `gamsel` fit regularization path. We then calculate BICs with respect to the regaluraization path and select the best $\lambda$, and adopt the corresponding fitted model. `getActive` will help get the estimated support.

```{r, message=FALSE}
library(gamsel)
gam = gamsel(X_train, y_train, family="binomial")
SUPPs = getActive(gam, c(1:50))
Ss = as.numeric(lapply(SUPPs, length))
Y_Fits = predict(gam, X_train, type="response")
LOSSes = apply(Y_Fits, 2, cross_entropy, y_true=y_train)
BICs = BIC(LOSSes, Ss, N)
best_idx = which.min(BICs)
supp_gam = getActive(gam, index=c(best_idx), type="nonlinear")[[1]]
train_err_gam = 1 - (sum((Y_Fits[, best_idx]>=0.5)*1==y_train)/300.)
Y_Preds = predict(gam, X_test, type="response")
test_err_gam = 1 - (sum((Y_Preds[, best_idx]>=0.5)*1==y_test)/300.)
```


The selected support is `r supp_gam`, the training error is `r train_err_gam`, the test error is `r test_err_gam`.

### Random Forest

In this section, we will implement Random Forest(RF) for predictive performance and variable importance. We will use R package `h2o`. For the installation, please see [_h2o_](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html)

```{r, include=FALSE}
library(h2o)
h2o.init() # start a h2o session
# Transfer data to h2o objects
data_train = as.h2o(cbind(X_train, y_train))
data_train["y_train"] = as.factor(data_train["y_train"])
data_test = as.h2o(cbind(X_test, y_test))
data_test["y_test"] = as.factor(data_test["y_test"])

rf = h2o.randomForest(y="y_train", training_frame=data_train)
supp_rf = c(1:500)[h2o.varimp(rf)$percentage>0.0125]
fit_rf = predict(rf, as.h2o(X_train))$predict
pred_rf = predict(rf, as.h2o(X_test))$predict
train_err_rf = 1 - sum(fit_rf==as.h2o(y_train))/300.
test_err_rf = 1 - sum(pred_rf==as.h2o(y_test))/300.
```

The selected support is `r supp_rf`, the training error is `r train_err_gam`, the test error is `r test_err_gam`.


### Bayesian Additive Regression Trees(BART)


```{r, include=FALSE}
options(java.parameters = "-Xmx5g") # reserve 5G memory
library(bartMachine)
set_bart_machine_num_cores(4)
bart_vs = bartMachine(X=X_train, y=(y_train), num_trees=75)
var_sel = var_selection_by_permute_cv(bart_vs)
bart = bartMachine(X=X_train, y=factor(y_train), num_trees=75)
supp_bart = var_sel$important_vars_cv
fit_bart = predict(bart, X_train, type="class")
pred_bart = predict(bart, X_test, type="class")
train_err_rf = 1 - sum(fit_bart==y_train)/300.
test_err_rf = 1 - sum(pred_bart==y_test)/300.
```










