---
title: "Linear_examples"
output: html_document
---

## SCAD penalty

In this section, we show how we did linear example analysis using SCAD penalty using `ncvreg` packages.

```{r pre-request, warning=FALSE}
library(ncvreg)
source("../../src/utils.R")
```

```{r settings}
k = 1
dirc = "../../data/linear/p_1000_N_1000_s_100/"
data = read_data(k, dirc)
N = dim(data$x)[1] # Sample size of training set
p = dim(data$x)[2] # dimension of training set
LAMBDAs = seq(0.05, 0.5, 0.05) # Candidate lambdas
BIC = c()
SUPP = list()
S = c()
```

In function `ncvreg`, we specify a single value of $\lambda$ to keep track of the BIC and number of variable $s$ for each individual $\lambda$.

```{r, warning=FALSE}
idx = 1
C = 1
for (lambda in LAMBDAs) {
    scad = ncvreg(data$x, data$y, penalty='SCAD', lambda=lambda)
    supp_x = which(scad$beta[2:1001] != 0)
    s = length(supp_x)
    loss = mean((predict(scad, as.matrix(data$x))-data$y)^2)
    bic = N*log(loss) + C*s*log(N)
    BIC = c(BIC, bic)
    S = c(S, s)
    SUPP[[idx]] = supp_x
    idx = idx + 1
}
plot(S, BIC)
```

In the following chunk, we provide the metric for the result

```{r}
best_lambda = LAMBDAs[which.min(BIC)]
best_supp = SUPP[[which.min(BIC)]]
best_scad = ncvreg(data$x, data$y, penalty='SCAD', lambda=best_lambda)
mse_train = mean((predict(best_scad, as.matrix(data$x)) - data$y)^2)
mse_test = mean((predict(best_scad, as.matrix(data$x_test)) - data$y_test)^2)
```