{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Selection\n",
    "## User Guide on nonlinear example\n",
    "In this notebook, we will demonstrate how to implement our method on our nonlinear example from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "from time import clock\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from torch.nn.parameter import Parameter\n",
    "from utils import data_load_n, measure, accuracy\n",
    "from models import Net_nonlinear\n",
    "from dfs import DFS_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We will load our data in the following chunk. The data, both covariates and response, need to be load as `pytorch` `Tensor` objects to be fed into the DFS algorithm. The function `data_load_n` will read in dataset and split it into training and test set so that both sets have same number of positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariates is of type: <class 'torch.Tensor'>\n",
      "The response is of type: <class 'torch.Tensor'>\n",
      "The dimension of training set: torch.Size([300, 500])\n",
      "    The number of positive sample: 150\n",
      "    The number of negative sample: 150\n",
      "The dimension of test set: torch.Size([300, 500])\n",
      "    The number of positive sample: 150\n",
      "    The number of negative sample: 150\n"
     ]
    }
   ],
   "source": [
    "# load and prepare datasets\n",
    "dirc = \"../../data/nonlinear/p_500_N_600_s_4/\"\n",
    "k = 0 # dataset number from 0 to 29\n",
    "X, Y, X_test, Y_test = data_load_n(k, directory=dirc)\n",
    "N, p = X.shape\n",
    "print(\"The covariates is of type:\", type(X))\n",
    "print(\"The response is of type:\", type(Y))\n",
    "print(\"The dimension of training set:\", X.shape)\n",
    "print(\"    The number of positive sample:\", len(np.where(Y==1)[0]))\n",
    "print(\"    The number of negative sample:\", len(np.where(Y==0)[0]))\n",
    "print(\"The dimension of test set:\", X.shape)\n",
    "print(\"    The number of positive sample:\", len(np.where(Y_test==1)[0]))\n",
    "print(\"    The number of negative sample:\", len(np.where(Y_test==0)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFS with fixed hyper-parameters\n",
    "In this section, we demonstrate how to run DFS with one given set of hyper-parameters. The hyper-parameters includes:\n",
    "* `s`, the number of variables to be selected;\n",
    "* `c`, the tunning parameters to control the magnitude of $\\lambda_1$ and $\\lambda_2$;\n",
    "* `epochs`, the number of DFS iterations to be run;\n",
    "* `n_hidden1` & `n_hidden2`, the number of neurons in the fully connect neural networks;\n",
    "* `learning_rate`, the learning rate for optimizer;\n",
    "* `Ts` & `step`, the parameters to control the optimization on given support\n",
    "\n",
    "Among the above hyper-parameters, `s` is the most important parameters, and the selection of $s$ will be demonstrated in next sections. `c` can be selection through a sequence of candidates that returns the smallest loss function. Others mostly are meant to help the convergence of the optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Support:  [2 0 3 1]\n",
      "Final Support:  [2 0 3 1]\n",
      "DFS algorithm finishs in 118.80000000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "# specify hyper-paramters\n",
    "s = 4\n",
    "c = 1\n",
    "epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 10\n",
    "learning_rate = 0.05\n",
    "Ts = 25 # To avoid long time waiting, this parameter has been shorten\n",
    "step = 5\n",
    "# Define Model\n",
    "torch.manual_seed(1) # set seed\n",
    "# Define a model with pre-specified structure and hyper parameters\n",
    "model = Net_nonlinear(n_feature=p, n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_output=2)\n",
    "# Define another model to save the current best model based on loss function value\n",
    "# The purpose is to prevent divergence of the training due to large learning rate or other reason\n",
    "best_model = Net_nonlinear(n_feature=p, n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_output=2)\n",
    "# Define optimizers for the optimization with given support\n",
    "# optimizer to separately optimize the hidden layers and selection layers\n",
    "# the selection layer will be optimized on given support only.\n",
    "# the optimzation of hidden layers and selection layer will take turn in iterations\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=learning_rate, weight_decay=0.0025*c)\n",
    "optimizer0 = torch.optim.Adam(model.hidden0.parameters(), lr=learning_rate, weight_decay=0.0005*c)\n",
    "# Define loss function\n",
    "lf = torch.nn.CrossEntropyLoss()\n",
    "# Allocated some objects to keep track of changes over iterations\n",
    "hist = []\n",
    "SUPP = []\n",
    "LOSSES = []\n",
    "supp_x = list(range(p)) # initial support\n",
    "SUPP.append(supp_x)\n",
    "### DFS algorithm\n",
    "start = clock()\n",
    "for i in range(epochs):\n",
    "    # One DFS epoch\n",
    "    model, supp_x, LOSS = DFS_epoch(model, s, supp_x, X, Y, lf, optimizer0, optimizer, Ts, step)\n",
    "    LOSSES = LOSSES + LOSS\n",
    "    supp_x.sort()\n",
    "    # Save current loss function value and support\n",
    "    hist.append(lf(model(X), Y).data.numpy().tolist())\n",
    "    SUPP.append(supp_x)\n",
    "    # Prevent divergence of optimization over support, save the current best model\n",
    "    if hist[-1] == min(hist):\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        best_supp = supp_x\n",
    "    # Early stop criteria\n",
    "    if len(SUPP[-1]) == len(SUPP[-2]) and (SUPP[-1] == SUPP[-2]).all():\n",
    "        break\n",
    "\n",
    "end = clock()\n",
    "print(\"DFS algorithm finishs in\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk, we will demonstrate the results from the DFS algorithm. Note that the training and test error is not the final results, as we recommend two-step procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The support selected is: [0 1 2 3]\n",
      "The index of non-zero coefficents on selection layer: [0 1 2 3]\n",
      "The training error is: 0.010000000000000009\n",
      "The test error is: 0.043333333333333335\n"
     ]
    }
   ],
   "source": [
    "# Metric calculation\n",
    "err_train_1 = 1-accuracy(best_model, X, Y)\n",
    "err_test_1 = 1-accuracy(best_model, X_test, Y_test)\n",
    "print(\"The support selected is:\", best_supp)\n",
    "print(\"The index of non-zero coefficents on selection layer:\", \n",
    "      np.where(best_model.hidden0.weight != 0)[0])\n",
    "print(\"The training error is:\", err_train_1)\n",
    "print(\"The test error is:\", err_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk, we will perform a two-step procedure to train the `best_model` on the given support. Two-step procedure is used for two reasons, to get better predictive performance and to get better estimation of `bic` which is important in selection of optimal $s$.\n",
    "\n",
    "As we demonstrated on the above chunk, the selection layer of `best_model` has non-zero coefficients on given support. In the second step, we treat `best_model` as our initial model and update parameters only in hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_optimizer = torch.optim.Adam(list(best_model.parameters())[1:], lr=0.01, weight_decay=0.0025)\n",
    "for _ in range(100):\n",
    "    out = best_model(X)\n",
    "    loss = lf(out, Y)\n",
    "    _optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    _optimizer.step()\n",
    "\n",
    "bic = (loss.data.numpy().tolist())*N*2. + s*np.log(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of $s$\n",
    "In this section, we demonstrate the procedure of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
