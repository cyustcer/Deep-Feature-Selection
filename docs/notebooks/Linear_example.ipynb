{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#User-Guide-on-linear-example\" data-toc-modified-id=\"User-Guide-on-linear-example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>User Guide on linear example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#DFS-with-fixed-hyper-parameters\" data-toc-modified-id=\"DFS-with-fixed-hyper-parameters-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>DFS with fixed hyper-parameters</a></span></li><li><span><a href=\"#Selection-of-$s$\" data-toc-modified-id=\"Selection-of-$s$-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Selection of $s$</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Selection\n",
    "In this notebook, we will demonstrate how to implement our method on the linear simulation examples from our paper.\n",
    "## User Guide on linear example\n",
    "In this example, a high dimensional dataset with 1000 covariates and 500 observations is generated using the linear system:\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "where $\\beta \\in \\mathbb{R}^{1000}$, but only the first 100 elements of $\\beta$ is non-zero. Our task is to correctly select the important variables. Please see section 5.1 of the paper for detailed generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "from time import clock\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from torch.nn.parameter import Parameter\n",
    "from utils import data_load_l, measure, mse\n",
    "from models import Net_linear\n",
    "from dfs import DFS_epoch, training_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We will load our data in the following chunk. \n",
    "\n",
    "The data, both covariates and response, need to be load as `pytorch` `Tensor` objects to be fed in to DFS algorithm. \n",
    "\n",
    "The covariates matrix of training set need to be $\\sqrt{n}$ column-wise normalized, $n$ is the sample size, i.e. summation of square of each column is $n$. \n",
    "\n",
    "The covariates matrix of testing set also need to be normalized correspondingly, i.e. divided by the some normalization constant of the training set. The function `data_load_l` will automatically read in dataset, normalizing, and split the dataset into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The covariates is of type: <class 'torch.Tensor'>\n",
      "The response is of type: <class 'torch.Tensor'>\n",
      "\n",
      "The dimension of training set: torch.Size([500, 1000])\n",
      "    The sum square of the first 5 columns:\n",
      "     tensor([499.9998, 500.0001, 500.0001, 499.9999, 499.9999])\n",
      "\n",
      "The dimension of test set: torch.Size([500, 1000])\n",
      "    The sum square of the first 5 columns:\n",
      "     tensor([467.6035, 536.6000, 467.3558, 435.5148, 476.9774])\n"
     ]
    }
   ],
   "source": [
    "# load and prepare datasets\n",
    "dirc = \"../../data/linear/p_1000_N_1000_s_100/\"\n",
    "k = 0 # dataset number from 0 to 9\n",
    "X, Y, X_test, Y_test, supp = data_load_l(k, directory=dirc)\n",
    "N, p = X.shape\n",
    "print(\"The covariates is of type:\", type(X))\n",
    "print(\"The response is of type:\", type(Y))\n",
    "print()\n",
    "print(\"The dimension of training set:\", X.shape)\n",
    "print(\"    The sum square of the first 5 columns:\")\n",
    "print(\"    \", torch.sum(X**2, dim=0)[:5])\n",
    "print()\n",
    "print(\"The dimension of test set:\", X_test.shape)\n",
    "print(\"    The sum square of the first 5 columns:\")\n",
    "print(\"    \", torch.sum(X_test**2, dim=0)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As displayed above, all columns of training set covariates matrix are normalized. The columns of test set covariates matrix is normalized by constant of training set, thus their sum square of columns are not unified. But since training set and test set follows the same distribution, they have very similar sum squares.\n",
    "\n",
    "### DFS with fixed hyper-parameters\n",
    "In this section, we demonstrate how to run DFS with one given set of hyper-parameters. The hyper-parameters includes:\n",
    "* `s`, the number of variables to be selected;\n",
    "* `c`, the tunning parameters to control the magnitude of $\\lambda_1$ and $\\lambda_2$;\n",
    "* `epochs`, the number of DFS iterations to be run;\n",
    "* `n_hidden1`, the number of neurons in the fully connect neural networks;\n",
    "* `learning_rate`, the learning rate for optimizer;\n",
    "* `Ts` & `step`, the parameters to control the optimization on given support\n",
    "\n",
    "Among the above hyper-parameters, `s` is the most important parameters, and the selection of $s$ will be demonstrated in next section. `c` can be selection through a sequence of candidates that returns the smallest loss function. Others mostly are meant to help the convergence of the optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyper-paramters\n",
    "s = 100\n",
    "c = 1\n",
    "epochs = 3 # We reduced the number of epochs in notebooks\n",
    "n_hidden1 = 1\n",
    "learning_rate = 0.001\n",
    "Ts = 1000\n",
    "step = 1\n",
    "\n",
    "# Define Model\n",
    "torch.manual_seed(1) # set seed \n",
    "# Define a model with pre-specified structure and hyper parameters\n",
    "model = Net_linear(n_feature=p, n_hidden1=n_hidden1, n_output=1)\n",
    "# Define another model to save the current best model based on loss function value\n",
    "# The purpose is to prevent divergence of the training due to large learning rate or other reason\n",
    "best_model = Net_linear(n_feature=p, n_hidden1=n_hidden1, n_output=1)\n",
    "\n",
    "# Define optimizers for the optimization with given support\n",
    "# optimizer to separately optimize the hidden layers and selection layers\n",
    "# the selection layer will be optimized on given support only.\n",
    "# the optimzation of hidden layers and selection layer will take turn in iterations\n",
    "optimizer = torch.optim.SGD(list(model.parameters()), lr=learning_rate, weight_decay=0.0025*c)\n",
    "optimizer0 = torch.optim.SGD(model.hidden0.parameters(), lr=learning_rate, weight_decay=0.0005*c)\n",
    "\n",
    "# Define loss function\n",
    "lf = torch.nn.MSELoss()\n",
    "\n",
    "# Allocated some objects to keep track of changes over iterations\n",
    "hist = []\n",
    "SUPP = []\n",
    "supp_x = list(range(p)) # initial support\n",
    "SUPP.append(supp_x)\n",
    "\n",
    "### DFS algorithm\n",
    "start = clock()\n",
    "for i in range(epochs):\n",
    "    # One DFS epoch\n",
    "    model, supp_x, _ = DFS_epoch(model, s, supp_x, X, Y, lf, optimizer0, optimizer, Ts, step)\n",
    "    supp_x.sort()\n",
    "    # Save current loss function value and support\n",
    "    hist.append(lf(model(X), Y).data.numpy().tolist())\n",
    "    SUPP.append(supp_x)\n",
    "    # Prevent divergence of optimization over support, save the current best model\n",
    "    if hist[-1] == min(hist):\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        best_supp = supp_x\n",
    "    # Early stop criteria\n",
    "    if len(SUPP[-1]) == len(SUPP[-2]) and len(set(SUPP[-1]).difference(SUPP[-2])) == 0:\n",
    "        break\n",
    "\n",
    "end = clock()\n",
    "print(\"Training finished in\", len(SUPP)-1, \"epochs, and took\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk, we will demonstrate the results from the DFS algorithm, in terms of selected support, number of missed or false selected support, training mse and test mse for __one step__ procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The support selected is: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
      "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  97  98  99 100 491 853]\n",
      "The index of non-zero coefficients on selection layer: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
      "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  97  98  99 100 491 853]\n",
      "\n",
      "Number of false selected variables: {100, 853, 491}\n",
      "Number of missed variables: {96, 58, 13}\n",
      "\n",
      "The training mse of one step is: 722.839599609375\n",
      "The test mse of one step is: 643.0396118164062\n"
     ]
    }
   ],
   "source": [
    "### metrics calculation\n",
    "fs = set(best_supp).difference(supp) # false selection number\n",
    "ns = set(supp).difference(best_supp) # negative selection number\n",
    "_err_train = mse(best_model, X, Y) # training error\n",
    "_err_test = mse(best_model, X_test, Y_test) # testing error\n",
    "\n",
    "print(\"The support selected is:\", best_supp)\n",
    "print(\"The index of non-zero coefficients on selection layer:\",\n",
    "      np.where(best_model.hidden0.weight != 0)[0])\n",
    "print()\n",
    "print(\"Number of false selected variables:\", fs)\n",
    "print(\"Number of missed variables:\", ns)\n",
    "print()\n",
    "print(\"The training mse of one step is:\", _err_train)\n",
    "print(\"The test mse of one step is:\", _err_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we have successfully selected most of the important variables without knowing the underling model and with the presence of correlation between variables.\n",
    "\n",
    "In the following chunk, we will perform a two-step procedure to train the `best_model` on the given support.\n",
    "\n",
    "Two-step procedure is used for two reasons, to get better predictive performance and to get better estimation of $bic$ which is important in selection of optimal s .\n",
    "\n",
    "As we demonstrated on the above chunk, the selection layer of `best_model` has non-zero coefficients on given support. In the second step, we treat `best_model` as our initial model and update parameters only in hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training mse of two step is: 1.040311336517334\n",
      "The test mse of two step is: 1.2704733610153198\n"
     ]
    }
   ],
   "source": [
    "_optimizer = torch.optim.Adam(list(best_model.parameters())[1:], lr=0.5)\n",
    "for _ in range(5000):\n",
    "    out = best_model(X)\n",
    "    loss = lf(out, Y)\n",
    "    _optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    _optimizer.step()\n",
    "    hist.append(loss.data.numpy().tolist())\n",
    "\n",
    "### metric calculation\n",
    "mse_train = mse(best_model, X, Y)\n",
    "mse_test = mse(best_model, X_test, Y_test)\n",
    "print(\"The training mse of two step is:\", mse_train)\n",
    "print(\"The test mse of two step is:\", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has shown that the predictive performance of our model is increased. \n",
    "\n",
    "All good results shown above is based on the correct given $s$. However, in reality, $s$ is unknown for most of the time. So the next thing would be finding the optimal $s$\n",
    "\n",
    "### Selection of $s$\n",
    "In this section, we demonstrate the procedure of selection of optimal $s$. We have wrapped up the training procedure above in a function `training_l`. For each given $s$,  $bic$, defined as $n \\cdot \\log \\hat{\\sigma}^2 + c \\cdot s \\cdot \\log n$, of the model will be automatically calculated by `training_l`, also the trained model with the given $s$ will also be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ss = [98, 99, 100, 101, 102]\n",
    "BIC = []\n",
    "for i, s in enumerate(Ss):\n",
    "    model, supp, bics, [err_train, err_test], _, _, _ = training_l(X, Y, X_test, Y_test, c, s\n",
    "                                                                   epochs=3, Ts=1000)\n",
    "    BIC.append(bics[1])\n",
    "    if bic == min(BIC):\n",
    "        best_model = model\n",
    "        best_supp = supp\n",
    "        best_err_train, best_err_test = err_train, err_test\n",
    "\n",
    "\n",
    "idx = np.argmin(BIC)\n",
    "best_s = Ss[idx]\n",
    "plt.plot(Ss, BIC)\n",
    "plt.axvline(x=best_s, ls='--', label=\"optimal s\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can tell $s=100$ is the optimal $s$, and the corresponding model is stored in `best_model` whose performance is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = set(best_supp).difference(supp)\n",
    "ns = set(supp).difference(best_supp)\n",
    "mse_train = mse(best_model, X, Y)\n",
    "mse_test = mse(best_model, X_test, Y_test)\n",
    "print(\"Number of false selected variables:\", fs)\n",
    "print(\"Number of missed variables:\", ns)\n",
    "print(\"The training mse of best model based on optimal s is:\", mse_train)\n",
    "print(\"The test mse of best model based on optimal s is:\", mse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
